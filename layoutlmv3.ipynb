{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2916806,"sourceType":"datasetVersion","datasetId":1569295}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LayoutLMv3-Invoice Extract: Fine-Tuning for Invoice Understanding\n\n**Project Idea:**  \nThis project focuses on fine-tuning the **LayoutLMv3** model on the SERIO dataset to improve invoice understanding and entity extraction. The primary goal is to enhance the model's ability to accurately interpret complex invoice layouts, identifying key information such as invoice numbers, dates, total amounts, and line items. By leveraging the LayoutLMv3 model's visual and textual learning capabilities, the project aims to achieve more efficient and accurate processing of invoices for real-world applications in financial management and automated data extraction.\n\n**Objectives:**\n- Fine-tune LayoutLMv3 on the SERIO dataset for better invoice understanding.\n- Improve the model's ability to recognize key entities such as invoice numbers, dates, total amounts, and line items.\n- Apply the model in real-world scenarios to automate invoice data extraction and financial management.\n\n**Conclusion:**  \nBy enhancing the LayoutLMv3 model's ability to accurately interpret invoices, this project seeks to advance automated solutions for financial document processing, leading to more streamlined workflows in financial and administrative tasks.\n","metadata":{}},{"cell_type":"markdown","source":"## Required Libraries and Imports\n\nThe following libraries and modules are essential for data processing, model training, evaluation, and visualization. They include utilities for handling datasets, image transformations, tokenization, and metrics calculation. The LayoutLMv3 model and Trainer from Hugging Faceâ€™s `transformers` library are also imported to facilitate token classification tasks on structured documents like invoices.\n\n- **os, glob, shutil**: For file handling and directory management.\n- **PIL (Python Imaging Library)**: For image processing and rendering.\n- **cv2 (OpenCV)**: For advanced image manipulation and visualization.\n- **torch, torchvision**: For building and training deep learning models.\n- **transformers**: To use the LayoutLMv3 model, tokenizers, and the Trainer class.\n- **sklearn**: To calculate metrics such as accuracy, precision, recall, and F1 score.\n- **matplotlib**: For visualizing images and bounding boxes.\n- **tqdm**: To display progress bars during training.","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport json \nimport random\nfrom pathlib import Path\nfrom difflib import SequenceMatcher\nimport shutil\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display\nimport matplotlib\nfrom matplotlib import pyplot, patches\nfrom time import perf_counter\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\nfrom transformers import LayoutLMv3Tokenizer\nfrom tqdm import tqdm\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import LayoutLMv3ForTokenClassification, AutoConfig\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### let's see one invoice to gain some insights","metadata":{}},{"cell_type":"code","source":"sroie_folder_path = Path('/kaggle/input/sroie-datasetv2/SROIE2019')\nexample_file = Path('X51005365187.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = Image.open(\"/kaggle/input/sroie-datasetv2/SROIE2019/train/img/X00016469612.jpg\")\nimage = image.convert(\"RGB\")\nnew_image = image.resize((300, 600))\nnew_image\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Prepocessing","metadata":{}},{"cell_type":"markdown","source":"## Reading Bounding Boxes and Words from Text Files\n\nThis function `read_bbox_and_words` reads bounding box coordinates and text from a given file and processes the data into a structured format. The text file is expected to contain comma-separated values where the first eight values represent the bounding box coordinates (in terms of four points: x0, y0, x1, y1, x2, y2, x3, y3) and the remaining values correspond to the associated text. The function splits these values, stores them in a list, and finally converts them into a Pandas DataFrame.\n\n### Steps:\n1. **File Reading and Parsing**: The function opens the specified file, processes each line, and extracts bounding box coordinates along with the corresponding text.\n2. **Data Storage**: The parsed data is stored in a list and then converted into a DataFrame for easier handling and manipulation.\n3. **Bounding Box Conversion**: The bounding box coordinates are explicitly converted into integers for future processing.\n4. **Dropping Unnecessary Columns**: Some of the bounding box columns are dropped to simplify the data (e.g., `x1`, `y1`, `x3`, and `y3` are removed).\n5. **Preview**: The function returns the processed DataFrame, and we display the first few rows of the file and the DataFrame to verify the result.\n\nThe `head()` command is used to show the first five lines from the input file and the first few rows of the resulting DataFrame.\n","metadata":{}},{"cell_type":"code","source":"def read_bbox_and_words(path: Path):\n  bbox_and_words_list = []\n\n  with open(path, 'r', errors='ignore') as f:\n    for line in f.read().splitlines():\n      if len(line) == 0:\n        continue\n        \n      split_lines = line.split(\",\")\n\n      bbox = np.array(split_lines[0:8], dtype=np.int32)\n      text = \",\".join(split_lines[8:])\n\n      # From the splited line we save (filename, [bounding box points], text line).\n      # The filename will be useful in the future\n      bbox_and_words_list.append([path.stem, *bbox, text])\n    \n  dataframe = pd.DataFrame(bbox_and_words_list, columns=['filename', 'x0', 'y0',\n                                                         'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'line'])\n\n  # Explicitly convert only the bounding box columns to integers\n  bbox_columns = ['x0', 'y0','x1', 'y1', 'x2', 'y2', 'x3', 'y3']  # Adjust based on your actual columns\n  dataframe[bbox_columns] = dataframe[bbox_columns].astype(np.int16)\n    \n  dataframe = dataframe.drop(columns=['x1', 'y1', 'x3', 'y3'])\n\n  return dataframe\nbbox_file_path = sroie_folder_path / \"test/box\" / example_file\nprint(\"== File content ==\")\n!head -n 5 \"{bbox_file_path}\"\n\nbbox = read_bbox_and_words(path=bbox_file_path)\nprint(\"\\n== Dataframe ==\")\nbbox.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Entities from JSON: \nThis function reads invoice entities from a JSON file and returns them as a Pandas DataFrame.\n","metadata":{}},{"cell_type":"code","source":"def read_entities(path: Path):\n  with open(path, 'r') as f:\n    data = json.load(f)\n\n  dataframe = pd.DataFrame([data])\n  return dataframe\n\n\n# Example usage\nentities_file_path = sroie_folder_path /  \"test/entities\" / example_file\nprint(\"== File content ==\")\n!head \"{entities_file_path}\"\n\nentities = read_entities(path=entities_file_path)\nprint(\"\\n\\n== Dataframe ==\")\nentities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Assign Line Label:\nThis function assigns a label to a line of text based on its similarity to entity names from a DataFrame, returning the matching entity type or \"O\" for no match.\n","metadata":{}},{"cell_type":"code","source":"def assign_line_label(line: str, entities: pd.DataFrame):\n    line_set = line.replace(\",\", \"\").strip().split()\n    for i, column in enumerate(entities):\n        entity_set =  entities.iloc[0, i].replace(\",\", \"\").strip().split()\n        \n        \n        matches_count = 0\n        for l in line_set:\n            if any(SequenceMatcher(a=l, b=b).ratio() > 0.8 for b in entity_set):\n                matches_count += 1\n            \n            if (column.upper() == 'ADDRESS' and (matches_count / len(line_set)) >= 0.5) or \\\n               matches_count == len(entity_set):\n                return column.upper()\n\n    return \"O\"\n\n\nline = bbox.loc[1,\"line\"]\nlabel = assign_line_label(line, entities)\nprint(\"Line:\", line)\nprint(\"Assigned label:\", label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Assign Labels:\nThis function assigns labels to words based on their bounding box dimensions and the presence of entities, ensuring unique assignments for critical fields like TOTAL and DATE while preventing conflicts.\n","metadata":{}},{"cell_type":"code","source":"def assign_labels(words: pd.DataFrame, entities: pd.DataFrame):\n    max_area = {\"TOTAL\": (0, -1), \"DATE\": (0, -1)}  # Value, index\n    already_labeled = {\"TOTAL\": False,\n                       \"DATE\": False,\n                       \"ADDRESS\": False,\n                       \"COMPANY\": False,\n                       \"O\": False\n    }\n\n    # Go through every line in $words and assign it a label\n    labels = []\n    for i, line in enumerate(words['line']):\n        label = assign_line_label(line, entities)\n\n        already_labeled[label] = True\n        if (label == \"ADDRESS\" and already_labeled[\"TOTAL\"]) or \\\n           (label == \"COMPANY\" and (already_labeled[\"DATE\"] or already_labeled[\"TOTAL\"])):\n            label = \"O\"\n         # Assign to the largest bounding box\n        if label in [\"TOTAL\", \"DATE\"]:\n            x0_loc = words.columns.get_loc(\"x0\")\n            bbox = words.iloc[i, x0_loc:x0_loc+4].to_list()\n            area = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n\n            if max_area[label][0] < area:\n                max_area[label] = (area, i)\n\n            label = \"O\"\n\n        labels.append(label)\n\n    labels[max_area[\"DATE\"][1]] = \"DATE\"\n    labels[max_area[\"TOTAL\"][1]] = \"TOTAL\"\n\n    words[\"label\"] = labels\n    return words\n\n\n# Example usage\nbbox_labeled = assign_labels(bbox, entities)\nbbox_labeled.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox_labeled.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Line:\nThis function splits a line into individual words while maintaining the same bounding box coordinates for each word, as the research indicates that they share the same context.\n","metadata":{}},{"cell_type":"code","source":"def split_line(line: pd.Series) -> list:\n    \"\"\"\n    Splits a line into words and updates bounding box coordinates for each word.\n    \n    Parameters:\n        line (pd.Series): A pandas Series with 'x0', 'x2', and 'line' columns.\n    \n    Returns:\n        list: A list of lists where each sublist contains updated values for the line.\n    \"\"\"\n    # Ensure the line has the necessary columns\n    if not {'x0', 'x2', 'line'}.issubset(line.index):\n        raise ValueError(\"The line must contain 'x0', 'x2', and 'line' columns.\")\n\n    # Extract current bounding box information\n    x0 = line['x0']\n    x2 = line['x2']\n    bbox_width = line['x2'] - line['x0']  # Example width, adjust as needed\n    line_str = line['line']\n\n    words = line_str.split()\n    new_lines = []\n\n    # Iterate through each word and calculate new bounding box coordinates\n    for index, word in enumerate(words):\n\n        # Create a new Series for the updated line\n        line_copy = line.copy()\n        line_copy['x0'] = x0\n        line_copy['x2'] = x2\n        line_copy['line'] = word\n        \n        # Append the updated line to the new_lines list\n        new_lines.append(line_copy.to_list())\n\n        # Update x0 for the next word\n    return new_lines\n\n\n\n# Example usage\nnew_lines = split_line(bbox_labeled.loc[1])\nprint(\"Original row:\")\ndisplay(bbox_labeled.loc[1:1,:])\n\nprint(\"Splitted row:\")\npd.DataFrame(new_lines, columns=bbox_labeled.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataset_creator(folder: Path):\n  bbox_folder = folder / 'box'\n  entities_folder = folder / 'entities'\n  img_folder = folder / 'img'\n\n  # Sort by filename so that when zipping them together\n  # we don't get some other file (just in case)\n  entities_files = sorted(entities_folder.glob(\"*.txt\"))\n  bbox_files = sorted(bbox_folder.glob(\"*.txt\"))\n  img_files = sorted(img_folder.glob(\"*.jpg\"))\n\n  data = []\n\n  print(\"Reading dataset:\")\n  for bbox_file, entities_file, img_file in tqdm(zip(bbox_files, entities_files, img_files), total=len(bbox_files)):            \n    # Read the files\n    bbox = read_bbox_and_words(bbox_file)\n    entities = read_entities(entities_file)\n    image = Image.open(img_file)\n\n    # Assign labels to lines in bbox using entities\n    bbox_labeled = assign_labels(bbox, entities)\n    del bbox\n\n    # Split lines into separate tokens\n    new_bbox_l = []\n    for index, row in bbox_labeled.iterrows():\n      new_bbox_l += split_line(row)\n    new_bbox = pd.DataFrame(new_bbox_l, columns=bbox_labeled.columns)\n    new_bbox[['x0', 'y0', 'x2', 'y2']] = new_bbox[['x0', 'y0', 'x2', 'y2']].astype(np.int16)\n\n    del bbox_labeled\n\n\n    # Do another label assignment to keep the labeling more precise \n    for index, row in new_bbox.iterrows():\n      label = row['label']\n\n      if label != \"O\":\n        entity_values = entities.iloc[0, entities.columns.get_loc(label.lower())]\n        entity_set = entity_values.split()\n        \n        if any(SequenceMatcher(a=row['line'], b=b).ratio() > 0.7 for b in entity_set):\n            label = \"S-\" + label\n        else:\n            label = \"O\"\n      \n      new_bbox.at[index, 'label'] = label\n\n    width, height = image.size\n  \n    data.append([new_bbox, width, height])\n\n  return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = dataset_creator(sroie_folder_path / 'train')\ndataset_test = dataset_creator(sroie_folder_path / 'test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train-test split","metadata":{}},{"cell_type":"code","source":"random.seed(42)\nrandom.shuffle(dataset_test)\ndataset_val = dataset_test[174:]\ndataset_test = dataset_test[:174]\nprint(len(dataset_val))\nprint (len(dataset_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train[0][0][\"x0\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom transformers import LayoutLMv3Tokenizer\nimport torch\n\nclass InvoiceDataset(Dataset):\n    def __init__(self, invoice_list, tokenizer, image_folder_path):\n        self.invoice_list = invoice_list\n        self.tokenizer = tokenizer\n        self.image_folder_path = image_folder_path\n        self.label_map = {\n            \"S-COMPANY\": 0,\n            \"S-ADDRESS\": 1,\n            \"S-DATE\": 2,\n            \"S-TOTAL\": 3,\n            \"O\": 4,  # For 'Other'\n        }\n        self.max_length = 512\n        \n        # Modify the transform to resize to 224x224, which aligns with model expectations\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),  # Resize to 224x224\n            transforms.ToTensor(),           \n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n        ])\n\n    def __len__(self):\n        return len(self.invoice_list)\n\n    def __getitem__(self, idx):\n        invoice_data = self.invoice_list[idx]\n        word_df = invoice_data[0]  \n        image_width = invoice_data[1]  \n        image_height = invoice_data[2]  \n        image_path = word_df[\"filename\"].iloc[0]  \n\n        words = []\n        bboxes = []\n        labels = []\n\n        # Load and preprocess the image\n        image_path = f\"{self.image_folder_path}/{image_path}.jpg\"\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {image_path}: {e}\")\n            return None  \n        \n        image = self.transform(image)  # Shape: [3, 224, 224]\n\n        # Add a batch dimension to the image tensor\n        image = image.unsqueeze(0)  # Now shape: [1, 3, 224, 224]\n\n        for _, word_data in word_df.iterrows():\n            word = word_data['line'] \n            label = word_data['label']\n            bbox = [\n                word_data['x0'],\n                word_data['y0'],\n                word_data['x2'],\n                word_data['y2']\n            ] \n\n            # Normalize the bounding boxes\n            normalized_bbox = [\n                bbox[0] * 1000 / image_width,\n                bbox[1] * 1000 / image_height,\n                bbox[2] * 1000 / image_width,\n                bbox[3] * 1000 / image_height\n            ]\n\n            words.append(word)\n            bboxes.append(normalized_bbox)  \n            labels.append(label)\n\n        # Tokenize the words with bounding boxes\n        tokens = self.tokenizer(\n            words,\n            boxes=bboxes,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            is_split_into_words=True,\n            return_tensors=\"pt\"\n        )\n\n        # Convert labels to numerical format\n        labels = [self.label_map.get(label, self.label_map[\"O\"]) for label in word_df['label'].tolist()]\n\n        # Pad labels to max length with -100\n        padded_labels = labels + [-100] * (self.max_length - len(labels))  # Use -100 for padding\n        labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n\n        # Ensure the bbox tensor is correctly padded\n        bbox_tensor = tokens['bbox'].squeeze(0)  \n        if bbox_tensor.size(0) < self.max_length:\n            padding = torch.zeros((self.max_length - bbox_tensor.size(0), 4), dtype=torch.float32)  # Pad with zeros\n            bbox_tensor = torch.cat([bbox_tensor, padding], dim=0)\n\n        # Convert everything to long \n        input_ids_tensor = tokens['input_ids'].squeeze(0).long()\n        attention_mask_tensor = tokens['attention_mask'].squeeze(0).long()\n\n        return  {\n            'input_ids': input_ids_tensor,\n            'attention_mask': attention_mask_tensor,\n            'bbox': bbox_tensor.to(torch.long),\n            'labels': labels_tensor,\n            'pixel_values': image.squeeze(0)  # Remove batch dimension for the final output shape [3, 224, 224]\n        }\n\n# Initialize tokenizer\ntokenizer = LayoutLMv3Tokenizer.from_pretrained(\"mp-02/layoutlmv3-large-cord2\")\n\n# Create your dataset\nimage_folder_path = '/kaggle/input/sroie-datasetv2/SROIE2019/train/img'\ndataset = InvoiceDataset(dataset_train, tokenizer=tokenizer, image_folder_path=image_folder_path)\nval_set = InvoiceDataset(dataset_val, tokenizer=tokenizer, image_folder_path=\"/kaggle/input/sroie-datasetv2/SROIE2019/test/img\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensure Tensor Shapes\nIt is crucial to verify that the shapes of all tensors are correct before proceeding with model training. This includes ensuring that input tensors match the expected dimensions of the model and that target tensors (labels) align with the input tensor shapes. Proper shape management helps prevent runtime errors and ensures the model learns effectively.\n","metadata":{}},{"cell_type":"code","source":"print(dataset[0][\"input_ids\"].shape)\nprint(dataset[0][\"attention_mask\"].shape)\nprint(dataset[0][\"bbox\"].shape)\nprint(dataset[0][\"labels\"].shape)\nprint(dataset[0][\"pixel_values\"].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0][\"bbox\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nfrom transformers import logging as transformers_logging\n\nwarnings.filterwarnings(\"ignore\")\ntransformers_logging.set_verbosity_error()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"43683e6439b3f848199c0e333e5ffdc8c1695604\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nIn this section, we define the training parameters and initialize the `Trainer` for fine-tuning the LayoutLMv3 model on the SERIO dataset. The training arguments include evaluation strategies, logging settings, and learning rate specifications. The model is configured to freeze the first few layers to retain pre-trained weights while allowing the rest to be trainable. The `compute_metrics` function is utilized to evaluate the model's performance during training.\n\nThe following code initializes the model and the training process.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom transformers import LayoutLMv3ForTokenClassification, Trainer, TrainingArguments\n\ndef compute_metrics(pred):\n    logits = pred.predictions\n    labels = pred.label_ids \n    \n    predictions = np.argmax(logits, axis=-1) \n\n    mask = labels != -100  \n    labels = labels[mask]\n    predictions = predictions[mask]\n\n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\n# Define class weights\nclass_weights = torch.tensor([5.0, 5.0, 5.0, 5.0, 1.0])  # Adjusted weights\n\nmodel = LayoutLMv3ForTokenClassification.from_pretrained(\n    \"mp-02/layoutlmv3-large-cord2\",\n    num_labels=5, \n    hidden_dropout_prob=0.2 \n)\n\nfor idx, param in enumerate(model.parameters()):\n    param.requires_grad = idx >= 8\n\ndef custom_loss_func(logits, labels):\n    loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)  # Use class weights here\n    return loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n\nmodel.loss_fct = custom_loss_func\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    logging_steps=100,\n    num_train_epochs=40,\n    learning_rate=1e-5,\n    report_to='wandb',\n    run_name='layoutlmv3-training',  \n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model='f1', \n    greater_is_better=True\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,  # Use the entire training dataset\n    eval_dataset=val_set,    # Use the entire validation dataset\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best Model Selection\n\nThe best model is the one with the highest F1 score on the validation set. This metric provides a balance between precision and recall, making it a suitable choice for evaluating model performance in tasks where class distribution may be imbalanced.\n","metadata":{}},{"cell_type":"code","source":"best_model = LayoutLMv3ForTokenClassification.from_pretrained('/kaggle/working/results/checkpoint-6573')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=best_model,\n    args=training_args,\n    compute_metrics=compute_metrics\n)\n\ntraining_eval = trainer.evaluate(eval_dataset=dataset)\nprint(\"Training Evaluation:\", training_eval)\n\nval_eval = trainer.evaluate(eval_dataset=val_set)\nprint(\"Validation Evaluation:\", val_eval)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating Model performance on the Test Set\n\nIn this section, we will evaluate the performance of our trained model on the test set.","metadata":{}},{"cell_type":"code","source":"test_set = InvoiceDataset(dataset_test, tokenizer=tokenizer, image_folder_path=\"/kaggle/input/sroie-datasetv2/SROIE2019/test/img\")\ntest_evaluation = trainer.evaluate(eval_dataset= test_set)\nprint(test_evaluation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Great Achievement!\nWe have achieved an impressive F1 score of **95.8%**! ðŸŽ‰","metadata":{}},{"cell_type":"markdown","source":"## Output Production Phase\n\nIn this phase of the pipeline, we aim to produce the output in JSON file format. This is crucial for integrating the model's predictions with other applications or systems that require structured data.\n\n### Steps to Produce JSON Output:\n\n1. **Extract Predictions**: After evaluating the model on the validation/test dataset, extract the relevant predictions (e.g., bounding boxes, labels, company name, date, address, and total).\n  \n2. **Structure the Data**: Organize the extracted data into a dictionary format. Each entry should correspond to a specific field that we want to include in the JSON output.\n\n3. **Convert to JSON**: Use Python's built-in `json` module to convert the structured data into JSON format.\n\n4. **Save the JSON File**: Write the JSON data to a file for further use or analysis.\n\nThis phase is typically referred to as the **Output Generation Phase** in the machine learning pipeline, where we focus on converting model predictions into a consumable format.\n","metadata":{}},{"cell_type":"markdown","source":"### I will use on observation from test set to extract the output as json file a","metadata":{}},{"cell_type":"code","source":"def generate_labels(sample)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    observation = {k: v for k, v in sample.items() if k != 'labels'}\n\n    input_ids = observation['input_ids'].unsqueeze(0)\n    attention_mask = observation['attention_mask'].unsqueeze(0)\n    bbox = observation['bbox'].unsqueeze(0)\n    pixel_values = observation['pixel_values'].unsqueeze(0)\n\n    best_model = best_model.to(device)\n\n    with torch.no_grad():\n        outputs = best_model(input_ids=input_ids, \n                        attention_mask=attention_mask, \n                        bbox=bbox, \n                        pixel_values=pixel_values)\n    return outputs\n\noutputs = generate_labels(test_set[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-21T00:10:09.438402Z","iopub.execute_input":"2024-10-21T00:10:09.438824Z","iopub.status.idle":"2024-10-21T00:10:09.447803Z","shell.execute_reply.started":"2024-10-21T00:10:09.438782Z","shell.execute_reply":"2024-10-21T00:10:09.446958Z"},"trusted":true},"execution_count":208,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[208], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def generate_labelsdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (563779616.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = dataset_test[0][0].drop([\"label\"], axis= 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_test[0][0]['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\ndef logits_to_labels(outputs, label_map: dict) -> pd.Series:\n    \"\"\"\n    Convert logits from TokenClassifierOutput to labels based on the label mapping.\n\n    Parameters:\n        outputs (TokenClassifierOutput): The output from the model containing logits.\n        label_map (dict): A mapping from label names to indices.\n\n    Returns:\n        pd.Series: A Pandas Series containing the predicted labels.\n    \"\"\"\n    # Extract logits from the outputs\n    logits = outputs.logits  # Access the logits attribute\n\n    # Get the predicted indices from the logits\n    predicted_indices = torch.argmax(logits, dim=-1)\n\n    # Create a reverse mapping from indices to labels\n    index_to_label = {v: k for k, v in label_map.items()}\n\n    # Map predicted indices to labels\n    # Use squeeze to remove unnecessary dimensions (batch size = 1 assumed)\n    predicted_labels = [index_to_label[idx.item()] for idx in predicted_indices.squeeze()]\n\n    # Create a Pandas Series from the predicted labels\n    labels_series = pd.Series(predicted_labels)\n\n    return labels_series\n\n# Example label mapping\nlabel_map = {\n    \"S-COMPANY\": 0,\n    \"S-ADDRESS\": 1,\n    \"S-DATE\": 2,\n    \"S-TOTAL\": 3,\n    \"O\": 4,  # For 'Other'\n}\n\n# Assuming 'outputs' is your TokenClassifierOutput object\n# Convert logits to labels\nlabels_series = logits_to_labels(outputs, label_map)\nprint(labels_series)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sample)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_output =pd.concat([sample, labels_series[:len(sample)]], axis= 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_output.columns = ['filename', 'x0', 'y0', 'x2', 'y2', 'line', 'label']  # Rename columns as needed\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter  # Ensure to import Counter\n\ndef reverse_words_and_vote(df: pd.DataFrame) -> pd.DataFrame:\n    # Group by bounding box coordinates\n    grouped = df.groupby(['x0', 'y0', 'x2', 'y2'])\n\n    final_labels = []\n    \n    for (x0, y0, x2, y2), group in grouped:\n        # Reverse the words in the same bounding box\n        reversed_words = ' '.join(reversed(group['line'].tolist()))\n        \n        # Count the occurrences of each label\n        label_counts = Counter(group['label'])\n        \n        # Get the most common label\n        most_common_label, count = label_counts.most_common(1)[0]\n        \n        # Add the reversed words along with bounding box coordinates and label to the final output\n        final_labels.append({\n            'filename': group['filename'].iloc[0],\n            'x0': x0,\n            'y0': y0,\n            'x2': x2,\n            'y2': y2,\n            'line': reversed_words,\n            'label': most_common_label,\n            'count': count\n        })\n\n    # Create a new DataFrame with the final results\n    final_df = pd.DataFrame(final_labels)\n\n    return final_df\n\n# Example usage\n# Assuming `sample_output` is your existing DataFrame\n# sample_output = pd.DataFrame(...)  # Your DataFrame goes here\nnew_sample_output = reverse_words_and_vote(sample_output)\n\n# Display the new DataFrame\nprint(new_sample_output.drop(\"count\", axis=1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_sample_output.drop([\"count\",\"filename\"], axis= 1).to_json(f\"/kaggle/working/sample_output.json\", orient='records', lines=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Path to your JSON file\njson_file_path = '/kaggle/working/sample_output.json'  # Update with your actual JSON file path\n\n# Initialize an empty list to store the loaded JSON data\ndata = []\n\ntry:\n    # Open and read the JSON file\n    with open(json_file_path, 'r') as file:\n        # Check if the file is line-delimited JSON (each line is a separate JSON object)\n        for line in file:\n            try:\n                # Load each line as a separate JSON object\n                data.append(json.loads(line))\n            except json.JSONDecodeError as e:\n                print(f\"Error decoding JSON on line: {line.strip()} - {e}\")\n\n    # If the file is not line-delimited, you can load it as a whole\n    # Uncomment this section if you expect the entire file to be a single JSON object or array\n    # data = json.load(file)\n\nexcept FileNotFoundError:\n    print(f\"File not found: {json_file_path}\")\nexcept json.JSONDecodeError as e:\n    print(f\"Error decoding JSON: {e}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Print the loaded data in a readable JSON format\ntry:\n    print(json.dumps(data, indent=4))  # Pretty print the JSON data\nexcept Exception as e:\n    print(f\"Error printing JSON data: {e}\")\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\n\nlabel_colors = {\n    \"S-COMPANY\": \"blue\",\n    \"S-ADDRESS\": \"red\",\n    \"S-DATE\": \"green\",\n    \"S-TOTAL\": \"orange\",\n    \"O\": \"black\"\n}\n\ndef draw_bounding_boxes(image_path, df):\n    try:\n        image = Image.open(image_path)\n        \n    except FileNotFoundError:\n        print(f\"Image file not found: {image_path}\")\n        return None\n\n    draw = ImageDraw.Draw(image)\n    \n    for index, row in df.iterrows():\n        x0, y0, x2, y2, label = row['x0'], row['y0'], row['x2'], row['y2'], row['label']\n        \n        color = label_colors.get(label, \"black\")\n        \n        draw.rectangle([x0, y0, x2, y2], outline=color, width=2)\n        \n    return image\n\nimage_filename = sample_output['filename'].iloc[0]\nimage_path = f\"/kaggle/input/sroie-datasetv2/SROIE2019/test/img/{image_filename}.jpg\"\n\noutput_image = draw_bounding_boxes(image_path, new_sample_output)\n\nif output_image is not None:\n    plt.figure(figsize=(10, 10))\n    plt.imshow(output_image)\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"No image to display.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_hub\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T23:53:06.436026Z","iopub.execute_input":"2024-10-20T23:53:06.436537Z","iopub.status.idle":"2024-10-20T23:53:17.778780Z","shell.execute_reply.started":"2024-10-20T23:53:06.436498Z","shell.execute_reply":"2024-10-20T23:53:17.777736Z"},"trusted":true},"execution_count":199,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Push Our Model to Hugging Face\n\nIn this section, we will upload our fine-tuned model to the Hugging Face Model Hub. This allows others to easily access and use our model.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T00:03:07.396190Z","iopub.execute_input":"2024-10-21T00:03:07.397184Z","iopub.status.idle":"2024-10-21T00:03:07.420198Z","shell.execute_reply.started":"2024-10-21T00:03:07.397120Z","shell.execute_reply":"2024-10-21T00:03:07.419548Z"},"trusted":true},"execution_count":205,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160e125d649f4ea289a6220eeec0883b"}},"metadata":{}}]},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nfrom huggingface_hub import HfApi, HfFolder\n\n# Step 1: Load Your Model Checkpoint\ncheckpoint_path = '/kaggle/working/results/checkpoint-6573'  # Update with your checkpoint path\nmodel = AutoModelForTokenClassification.from_pretrained(checkpoint_path)\n\n\n\nmodel_name = \"MohmaedElnamir/fine-tuned-layoutlmv3-sroie\"  # Update with your Hugging Face username and desired model name\n\n\nmodel.push_to_hub(model_name)\n\n\nprint(f\"Model uploaded to Hugging Face: {model_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T00:03:28.509491Z","iopub.execute_input":"2024-10-21T00:03:28.509882Z","iopub.status.idle":"2024-10-21T00:04:42.163619Z","shell.execute_reply.started":"2024-10-21T00:03:28.509842Z","shell.execute_reply":"2024-10-21T00:04:42.163011Z"},"trusted":true},"execution_count":206,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa3f65310acc43bd9a11ba210b504394"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer_name = \"mp-02/layoutlmv3-large-cord2\"  # Replace with the original model name you used\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\nmodel.push_to_hub(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T00:05:59.749859Z","iopub.execute_input":"2024-10-21T00:05:59.750544Z","iopub.status.idle":"2024-10-21T00:06:07.732692Z","shell.execute_reply.started":"2024-10-21T00:05:59.750505Z","shell.execute_reply":"2024-10-21T00:06:07.731941Z"},"trusted":true},"execution_count":207,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5af37c0f86e484cbca96b5422f94b08"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":207,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/MohmaedElnamir/fine-tuned-layoutlmv3-sroie/commit/a0f4d957af28c85155a0fc5a5979e7739a029115', commit_message='Upload LayoutLMv3ForTokenClassification', commit_description='', oid='a0f4d957af28c85155a0fc5a5979e7739a029115', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MohmaedElnamir/fine-tuned-layoutlmv3-sroie', endpoint='https://huggingface.co', repo_type='model', repo_id='MohmaedElnamir/fine-tuned-layoutlmv3-sroie'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}